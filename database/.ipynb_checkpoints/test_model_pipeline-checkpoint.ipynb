{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_dataset.csv\", dtype = {\"cnae\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _h0, _h1, _h2\n",
    "# _h0: history 0, here h0 means the year 2017 (historia 0, aquí h0 significa el año 2017)\n",
    "# _h1: history -1, here h1 means the year 2016 (historia -1, aquí h1 significa el año 2016)\n",
    "# _h2: history -2, here h2 means the year 2015 (historia -2, aquí h2 significa el año 2015)\n",
    "\n",
    "# Ebita Margin - Ebitda / Turn over (Ventas)\n",
    "# p49100: Profit (Resultado del ejercicio)\n",
    "# p40800: Amortization (Amortización) \n",
    "# p40100: Sales Turnover (Ingresos de Explotación)\n",
    "# p40500: Other sales (Otros Ingresos)\n",
    "df['ebitda_income'] = (df.p49100_h1+df.p40800_h1)/(df.p40100_mas_40500_h1) \n",
    "\n",
    "# Total Debt / Ebita \n",
    "# p31200: Short Term Debt / Deuda a corto plazo\n",
    "# p32300: Long Term Debt / Deuda a largo plazo\n",
    "# p49100: Profit (Resultado del ejercicio)\n",
    "# p40800: Amortization (Amortización) \n",
    "df['debt_ebitda'] =(df.p31200_h1 + df.p32300_h1) /(df.p49100_h1+df.p40800_h1) \n",
    "\n",
    "# rraa_rrpp: Financial leveraging / apalancamiento financiero \n",
    "# p10000: Total Assets / Total activos\n",
    "# p20000: Own Capital / Patrimonio neto\n",
    "df['rraa_rrpp'] = (df.p10000_h1 - df.p20000_h1) /df.p20000_h1\n",
    "\n",
    "# Log of Operating Income\n",
    "df['log_operating_income'] = np.log(df.p40100_mas_40500_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[['ebitda_income','debt_ebitda','rraa_rrpp','log_operating_income','target_status', 'cnae']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "X = df_clean[['ebitda_income','debt_ebitda','rraa_rrpp','log_operating_income', 'cnae']]\n",
    "y = df_clean['target_status']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30247\n",
       "1     3582\n",
       "Name: target_status, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.target_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a clear problem of an imbalanced dataset, so we have to deal with some techniques in order to try to score better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    22726\n",
       "0    22726\n",
       "Name: target_status, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X = df_clean[['ebitda_income','debt_ebitda','rraa_rrpp','log_operating_income', 'cnae']]\n",
    "y = df_clean['target_status']\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "active = X[X.target_status==0]\n",
    "closed = X[X.target_status==1]\n",
    "\n",
    "# upsample minority\n",
    "closed_upsampled = resample(closed,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(active), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([active, closed_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.target_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = upsampled.drop(columns = [\"target_status\"])\n",
    "y_train = upsampled.target_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = [\"sector\"]\n",
    "\n",
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['ebitda_income','debt_ebitda','rraa_rrpp','log_operating_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNAE_Transformer(BaseEstimator, TransformerMixin ):   \n",
    "\n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y = None  ):\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X , y = None ):  \n",
    "        X = X.copy()\n",
    "        X.loc[:, \"sector\"] = X.cnae.str[:2]\n",
    "        X.sector = X.sector.str.strip()\n",
    "        X = X.replace({\"sector\":\"\"}, \"missing\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean_Imputer(BaseEstimator, TransformerMixin ):   \n",
    "\n",
    "    #Return self nothing else to do here\n",
    "    def fit( self, X, y = None  ):\n",
    "        return self\n",
    "    \n",
    "    #Transformer method we wrote for this transformer \n",
    "    def transform(self, X , y = None ):  \n",
    "        numeric_column_names = X.select_dtypes(include =[\"float64\", \"int\"]).columns\n",
    "        X = X.copy()\n",
    "        X[numeric_column_names] = X[numeric_column_names].fillna(X.mean())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNormalizer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class used for imputing missing values in a pd.DataFrame using either mean or median of a group.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    group_cols : list\n",
    "        List of columns used for calculating the aggregated value \n",
    "    target : str\n",
    "        The name of the column to impute\n",
    "    metric : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like\n",
    "        The array with imputed values in the target column\n",
    "    '''\n",
    "    def __init__(self, group_cols, target):\n",
    "        \n",
    "        self.group_cols = group_cols\n",
    "        self.target = target\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        assert pd.isnull(X[self.group_cols]).any(axis=None) == False, 'There are missing values in group_cols'\n",
    "        \n",
    "        impute_map = X.groupby(self.group_cols)[self.target].agg([np.mean, np.std]) \\\n",
    "                                                            .reset_index(drop=False)\n",
    "        self.impute_map_ = impute_map.fillna(impute_map.median())\n",
    "        \n",
    "        impute_map_total = X[self.target].agg([np.mean, np.std]) \n",
    "        self.impute_map_total = impute_map_total.fillna(impute_map_total.median())        \n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def normalizer_sector(self, df):\n",
    "        \n",
    "        df_normalized = pd.DataFrame(columns = df.columns)        \n",
    "        for group, x in df.groupby(\"sector\"):\n",
    "            if any(x.sector.isin(self.impute_map_.sector)):\n",
    "                impute_sector = self.impute_map_.loc[self.impute_map_.sector.isin(x.sector)]\n",
    "\n",
    "                mean = impute_sector.xs(\"mean\", level = 1, axis = 1)\n",
    "                std = impute_sector.xs(\"std\", level = 1, axis = 1)\n",
    "                x[self.target] = (x[self.target] - mean.iloc[0]) -(x[self.target] - std.iloc[0])\n",
    "            else:\n",
    "                x.loc[:, self.target] = (x[self.target] - self.impute_map_total.loc['mean']) / self.impute_map_total.loc['std']\n",
    "            df_normalized = df_normalized.append(x)               \n",
    "        return df_normalized\n",
    "    \n",
    "    def normalizer_total(self, df):\n",
    "        \n",
    "        df.loc[:, self.target] = (df[self.target] - self.impute_map_total.loc['mean']) / self.impute_map_total.loc['std']\n",
    "        return df\n",
    "        \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # make sure that the imputer was fitted\n",
    "        check_is_fitted(self, 'impute_map_')\n",
    "        \n",
    "        X = X.copy()\n",
    "        df_final = pd.DataFrame(columns = X.columns)\n",
    "        \n",
    "        # Primero vemos si el sector de la tupla que queremos trasnformar está en el atributo impute_map_, que contiene\n",
    "        # la media y la desviación estándar por grupo de cuando se entrenó el modelo. Si no está, aplicamos la normalización\n",
    "        # basándonos en la media y la desviación estándar de todo el dataset con el que se entrenó.\n",
    "\n",
    "        df_final = self.normalizer_sector(X)\n",
    "            \n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    #Class Constructor \n",
    "    def __init__( self, feature_names ):\n",
    "        self._feature_names = feature_names \n",
    "    \n",
    "    #Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    #Method that describes what we need this transformer to do\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[ self._feature_names ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([(\"CNAE_Transformer\", CNAE_Transformer()), (\"Mean_Imputer\", Mean_Imputer()), (\"standarize\", GroupNormalizer([\"sector\"], numerical_features))])\n",
    "categorical_pipeline = Pipeline( steps = [ ( 'preprocessing', preprocessing ), \n",
    "                                  ( 'cat_selector', FeatureSelector(categorical_features)),\n",
    "                                  ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "\n",
    "numerical_pipeline = Pipeline( steps = [( 'preprocessing', preprocessing ), \n",
    "                                  ( 'cat_selector', FeatureSelector(numerical_features))])\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "                                                  ( 'numerical_pipeline', numerical_pipeline ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.8911159024905395\n",
      "Accuracy test: 0.41948451170489476\n"
     ]
    }
   ],
   "source": [
    "full_pipeline_m = Pipeline( steps = [ ( 'full_pipeline', full_pipeline),\n",
    "                                  \n",
    "                                  ( 'model', RandomForestClassifier(random_state=1234)) ] )\n",
    "\n",
    "\n",
    "full_pipeline_m.fit(X_train, y_train )\n",
    "\n",
    "\n",
    "y_pred = full_pipeline_m.predict( X_train ) \n",
    "y_pred_test = full_pipeline_m.predict( X_test ) \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy train: {0}\".format(accuracy_score(y_pred,y_train)))\n",
    "\n",
    "print(\"Accuracy test: {0}\".format(accuracy_score(y_pred_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report for train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88     22726\n",
      "           1       0.82      1.00      0.90     22726\n",
      "\n",
      "    accuracy                           0.89     45452\n",
      "   macro avg       0.91      0.89      0.89     45452\n",
      "weighted avg       0.91      0.89      0.89     45452\n",
      "\n",
      "classification report for test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.39      0.55      7521\n",
      "           1       0.11      0.63      0.19       937\n",
      "\n",
      "    accuracy                           0.42      8458\n",
      "   macro avg       0.50      0.51      0.37      8458\n",
      "weighted avg       0.81      0.42      0.51      8458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"classification report for train\")\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"classification report for test\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2645\n",
       "0    2645\n",
       "Name: target_status, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downsample minority\n",
    "active_downsample = resample(active,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(closed), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and downsample minority\n",
    "downsample = pd.concat([active_downsample, closed])\n",
    "\n",
    "# check new class counts\n",
    "downsample.target_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = downsample.drop(columns = [\"target_status\"])\n",
    "y_train = downsample.target_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([(\"CNAE_Transformer\", CNAE_Transformer()), (\"Mean_Imputer\", Mean_Imputer()), (\"standarize\", GroupNormalizer([\"sector\"], numerical_features))])\n",
    "categorical_pipeline = Pipeline( steps = [ ( 'preprocessing', preprocessing ), \n",
    "                                  ( 'cat_selector', FeatureSelector(categorical_features)),\n",
    "                                  ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "\n",
    "numerical_pipeline = Pipeline( steps = [( 'preprocessing', preprocessing ), \n",
    "                                  ( 'cat_selector', FeatureSelector(numerical_features))])\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "                                                  ( 'numerical_pipeline', numerical_pipeline ) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.8982986767485822\n"
     ]
    }
   ],
   "source": [
    "full_pipeline_m = Pipeline( steps = [ ( 'full_pipeline', full_pipeline),\n",
    "                                  \n",
    "                                  ( 'model', RandomForestClassifier(random_state=1234)) ] )\n",
    "\n",
    "\n",
    "full_pipeline_m.fit(X_train, y_train )\n",
    "\n",
    "\n",
    "y_pred = full_pipeline_m.predict( X_train ) \n",
    "y_pred_test = full_pipeline_m.predict( X_test ) \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy train: {0}\".format(accuracy_score(y_pred,y_train)))\n",
    "\n",
    "#print(\"Accuracy test: {0}\".format(accuracy_score(y_pred_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report for train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89      2645\n",
      "           1       0.83      1.00      0.91      2645\n",
      "\n",
      "    accuracy                           0.90      5290\n",
      "   macro avg       0.92      0.90      0.90      5290\n",
      "weighted avg       0.92      0.90      0.90      5290\n",
      "\n",
      "classification report for test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.39      0.55      7521\n",
      "           1       0.11      0.63      0.19       937\n",
      "\n",
      "    accuracy                           0.42      8458\n",
      "   macro avg       0.50      0.51      0.37      8458\n",
      "weighted avg       0.81      0.42      0.51      8458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report for train\")\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print(\"classification report for test\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0b052d41b57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
